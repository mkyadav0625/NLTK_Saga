{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Text Language by Counting Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature e.g. search engines remove words like 'want'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.tokenize import wordpunct_tokenize \n",
    "# RE-based tokenizer which splits text on whitespace and punctuation (except for underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yo',\n",
       " 'man',\n",
       " ',',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'time',\n",
       " 'for',\n",
       " 'you',\n",
       " 'to',\n",
       " 'shut',\n",
       " 'yo',\n",
       " \"'\",\n",
       " 'mouth',\n",
       " '!',\n",
       " 'I',\n",
       " 'ain',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'messin',\n",
       " \"'\",\n",
       " 'dawg',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = wordpunct_tokenize(text)\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other tokenizers e.g. RegexpTokenizer where we can enter your own regexp, WhitespaceTokenizer (similar to Python's  string.split()) and BlanklineTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploring NLTK's stop words corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes with a corpus of stop words in various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ') \n",
    "# Since this is raw text, we need to replace \\n's with spaces for it to be readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()\n",
    "# Most corpora consist of a set of files, each containing a piece of text. \n",
    "# A list of identifiers for these files is accessed via fileids()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus readers provide a variety of methods to read data from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'au\\naux\\navec\\nce\\nces\\ndans\\nde\\ndes\\ndu\\nelle\\nen\\net\\neux\\nil\\nje\\nla\\nle\\nleur\\nlui\\nma\\nmais\\nme\\nmême\\nmes\\nmoi\\nmon\\nne\\nnos\\nnotre\\nnous\\non\\nou\\npar\\npas\\npour\\nqu\\nque\\nqui\\nsa\\nse\\nses\\nson\\nsur\\nta\\nte\\ntes\\ntoi\\nton\\ntu\\nun\\nune\\nvos\\nvotre\\nvous\\nc\\nd\\nj\\nl\\nà\\nm\\nn\\ns\\nt\\ny\\nété\\nétée\\nétées\\nétés\\nétant\\nétante\\nétants\\nétantes\\nsuis\\nes\\nest\\nsommes\\nêtes\\nsont\\nserai\\nseras\\nsera\\nserons\\nserez\\nseront\\nserais\\nserait\\nserions\\nseriez\\nseraient\\nétais\\nétait\\nétions\\nétiez\\nétaient\\nfus\\nfut\\nfûmes\\nfûtes\\nfurent\\nsois\\nsoit\\nsoyons\\nsoyez\\nsoient\\nfusse\\nfusses\\nfût\\nfussions\\nfussiez\\nfussent\\nayant\\nayante\\nayantes\\nayants\\neu\\neue\\neues\\neus\\nai\\nas\\navons\\navez\\nont\\naurai\\nauras\\naura\\naurons\\naurez\\nauront\\naurais\\naurait\\naurions\\nauriez\\nauraient\\navais\\navait\\navions\\naviez\\navaient\\neut\\neûmes\\neûtes\\neurent\\naie\\naies\\nait\\nayons\\nayez\\naient\\neusse\\neusses\\neût\\neussions\\neussiez\\neussent\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.raw('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'au aux avec ce ces dans de des du elle en et eux il je la le leur lui ma mais me même mes moi mon ne nos notre nous on ou par pas pour qu que qui sa se ses son sur ta te tes toi ton tu un une vos votre vous c d j l à m n s t y été étée étées étés étant étante étants étantes suis es est sommes êtes sont serai seras sera serons serez seront serais serait serions seriez seraient étais était étions étiez étaient fus fut fûmes fûtes furent sois soit soyons soyez soient fusse fusses fût fussions fussiez fussent ayant ayante ayantes ayants eu eue eues eus ai as avons avez ont aurai auras aura aurons aurez auront aurais aurait aurions auriez auraient avais avait avions aviez avaient eut eûmes eûtes eurent aie aies ait ayons ayez aient eusse eusses eût eussions eussiez eussent '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading it Better\n",
    "stopwords.raw('french').replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use .sents() which returns sentences. However, in our particular case, this will cause an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordListCorpusReader' object has no attribute 'sents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5073a65ac8e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordListCorpusReader' object has no attribute 'sents'"
     ]
    }
   ],
   "source": [
    "stopwords.sents('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is because the stopwords corpus reader is of type WordListCorpusReader so there are no sentences. It's the same for .paras()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['english'])) # There is a total of 179 English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['Greek'])) # There is a total of 265 Greek stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['english', 'greek'])) # There is a total of 444 Greek and English stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " 'ain',\n",
       " 'dawg',\n",
       " 'even',\n",
       " 'for',\n",
       " 'i',\n",
       " 'it',\n",
       " 'man',\n",
       " 'messin',\n",
       " 'mouth',\n",
       " 's',\n",
       " 'shut',\n",
       " 't',\n",
       " 'time',\n",
       " 'to',\n",
       " 'yo',\n",
       " 'you'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_ratios = {}\n",
    "\n",
    "test_words = [word.lower() for word in test_tokens] # lowercase all tokens\n",
    "test_words_set = set(test_words)\n",
    "test_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in stopwords.fileids():\n",
    "    stopwords_set = set(stopwords.words(language)) \n",
    "    # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
    "    common_elements = test_words_set.intersection(stopwords_set)\n",
    "    language_ratios[language] = len(common_elements) # language \"score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 0,\n",
       " 'danish': 3,\n",
       " 'dutch': 0,\n",
       " 'english': 8,\n",
       " 'finnish': 0,\n",
       " 'french': 2,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 0,\n",
       " 'italian': 1,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 3,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 2,\n",
       " 'russian': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 2,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets view the ratio\n",
    "language_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The key parameter to the max() function is a function that computes a key. In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\n",
    "most_rated_language = max(language_ratios, key=language_ratios.get) \n",
    "most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ain', 'for', 'i', 'it', 's', 't', 'to', 'you'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words_set.intersection(set(stopwords.words(most_rated_language))) # We can see which English stop words were found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
